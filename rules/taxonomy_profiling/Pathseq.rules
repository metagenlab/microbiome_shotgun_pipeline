rule all_pathseq:
    input: expand("Pathseq/{sample}/output/scores.txt",sample=list(read_naming.keys())),
           #expand("Pathseq/{sample}/output/krona.html",sample=list(read_naming.keys()))


rule pathseq_fastq_to_bam_PE_reads:
    singularity: "docker://broadinstitute/gatk:4.1.3.0"

    input: r1=expand("unmapped/{host_acc}/{{sample}}_R1.fq",host_acc=config['reference_genome']['filename']),
            r2=expand("unmapped/{host_acc}/{{sample}}_R2.fq",host_acc=config['reference_genome']['filename'])

    log: logging_folder+"Pathseq/{sample}/FastqToSam.log"

    output: "Pathseq/{sample}/PE_input_reads.bam"

    threads: 10

    shell:
        """
        gatk --java-options "-Xmx32G -XX:ConcGCThreads=10" FastqToSam -F1 {input.r1} -F2 {input.r2} -O {output} -SM {wildcards.sample} &> {log}
        """

rule pathseq_fastq_to_bam_SE_reads:
    singularity: "docker://broadinstitute/gatk:4.1.3.0"

    input: r1=expand("unmapped/{host_acc}/{{sample}}_single.fq",host_acc=config['reference_genome']['filename'])

    log: logging_folder+"Pathseq/{sample}/FastqToSam.log"

    output: "Pathseq/{sample}/SE_input_reads.bam"

    threads: 10

    shell:
        """
        gatk --java-options "-Xmx32G -XX:ConcGCThreads=10" FastqToSam -F1 {input.r1} -O {output} -SM {wildcards.sample} &> {log}
        """

db_path=config["pathseq_database"]

rule build_host_kmer:
    singularity: "docker://broadinstitute/gatk:4.1.3.0"

    input: "host/{host_acc}.fa"

    output: f"{db_path}/custom/{{host_acc}}.bfi"

    params: kmer_size=config["kmer_size"],kmer_mask=config["kmer_mask"]

    shell: 'gatk PathSeqBuildKmers -R {input} -O {output} -P 0.0001 --java-options "-Xmx60g"'

def get_bam_input(wildcards):
    fastq_list = read_naming[wildcards.sample]
    decontaminated=config["filter_with_Pathseq"]
    if len(fastq_list) == 2 and not decontaminated :
        return "Pathseq/{sample}/PE_input_reads.bam"
    if len(fastq_list) == 1 and not decontaminated:
        return "Pathseq/{sample}/SE_input_reads.bam"
    if decontaminated:
        return expand("Pathseq/{sample}/filtered_reads/Paired_reads.bam","Pathseq/{sample}/filtered_reads/Unpaired_reads.bam")



rule pathseq_on_reads:
    singularity: "docker://broadinstitute/gatk:4.1.3.0"

    input: bam=get_bam_input,
           host_kmer=f"{db_path}/pathseq_host.bfi",
           host_bwa=f"{db_path}/pathseq_host.fa.img",
           microbe_fa=f"{db_path}/pathseq_microbe.fa",
           microbe_bwa=f"{db_path}/pathseq_microbe.fa.img",
           microbe_dict=f"{db_path}/pathseq_microbe.dict",
           tax_db=f"{db_path}/pathseq_taxonomy.db"

    output: score_out="Pathseq/{sample}/output/scores.txt",
            bam_out="Pathseq/{sample}/output/output_reads.bam",
            filter_met="Pathseq/{sample}/output/filter_metrics.txt",
            score_met="Pathseq/{sample}/output/score_metrics.txt"

    params: min_clip_len=config["min_clip_len"],
            min_score=config["min_score_identity"],
            margin=config["identity_margin"],
            skip_host_filter=config["skip_host_align"],
            divide_by_gen_len=config["divide_by_gen_len"],
            mem_limit="32G"

    threads: config["num_cores"]

    log: logging_folder+"Pathseq/{sample}/PathseqPipelineSpark.log"

    benchmark: "benchmarks/resources_usage/{sample}/pathseq.benchmark.txt"

    shell:
        """
        gatk PathSeqPipelineSpark --java-options "-Xmx{params.mem_limit}" --input {input.bam} \
                                  --kmer-file {input.host_kmer} \
                                  --filter-bwa-image {input.host_bwa} \
                                  --microbe-bwa-image {input.microbe_bwa} \
                                  --microbe-fasta {input.microbe_fa} \
                                  --taxonomy-file {input.tax_db} \
                                  --min-clipped-read-length {params.min_clip_len} \
                                  --min-score-identity {params.min_score} \
                                  --identity-margin {params.margin} \
                                  --scores-output {output.score_out} \
                                  --output {output.bam_out} \
                                  --filter-metrics {output.filter_met} \
                                  --score-metrics {output.score_met} \
                                  --is-host-aligned {params.skip_host_filter} \
                                  --divide-by-genome-length {params.divide_by_gen_len} \
                                  --conf 'spark.executor.cores={threads}' 2> {log}
        """

rule Pathseq_to_krona:
    input: "Pathseq/{sample}/output/scores.txt"

    output: temp("Pathseq/{sample}/output/krona.txt")

    script: "scripts/Pathseq2Krona.py"

rule krona_plot:
    conda: pipeline_path + "envs/krona.yml"
    
    input: "Pathseq/{sample}/output/krona.txt"

    output: "Pathseq/{sample}/output/krona.html"

    shell: "ktImportText -o {output} {input}"