rank=config["target_rank"]
rule all_tools_output:
    input: merged_tsv=expand("benchmark_tools/tables/{tool}/all_{tool}.tsv",tool=['kraken2','bracken','pathseq','kaiju','surpi','ezvir','ganon']),
            all_hetmaps=expand("benchmark_tools/heatmaps/all_{tool}.pdf",tool=['kraken2','bracken','pathseq','kaiju','surpi','ezvir','ganon']),
            gold_standard="gold_standard/tables/gold_standard.tsv",
            tool_scores=f"benchmark_tools/stats/all_tools_{rank}_level.tsv"


rule merge_Kraken2_outputs:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: expand("kraken2/{sample}/report.txt",sample=list(read_naming.keys()))

    output: "benchmark_tools/tables/kraken2/all_kraken2.tsv"

    script: "scripts/kraken2-merge.py"

rule merge_Bracken_outputs:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: expand("kraken2/{sample}/bracken.txt",sample=list(read_naming.keys()))

    output: "benchmark_tools/tables/bracken/all_bracken.tsv"

    script: "scripts/bracken-merge.py"

rule merge_Pathseq_outputs:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: expand("Pathseq/{sample}/output/scores.txt",sample=list(read_naming.keys()))

    output: "benchmark_tools/tables/pathseq/all_pathseq.tsv"

    script: "scripts/pathseq-merge.py"

rule merge_kaiju_outputs:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: expand("kaiju/{sample}/summary.tsv",sample=list(read_naming.keys()))

    output: "benchmark_tools/tables/kaiju/all_kaiju.tsv"

    log: logging_folder + "benchmark_tools/kaiju.log"

    script: "scripts/kaiju-merge.py"

rule merge_SURPI_outputs:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: bac=expand("SURPI/{sample}/OUTPUT_{sample}/{sample}.NT.snap.matched.d1.fl.Bacteria.annotated.species.clx.counttable",sample=list(read_naming.keys())),
           vir=expand("SURPI/{sample}/OUTPUT_{sample}/{sample}.NT.snap.matched.d16.fl.Viruses.filt.NTblastn_tru.dust.annotated.species.clx.counttable",sample=list(read_naming.keys()))
            #dust.annotated file because low complexity regions are substracted

    output: "benchmark_tools/tables/surpi/all_surpi.tsv"

    log: logging_folder + "benchmark_tools/surpi.log"

    script: "scripts/surpi-merge.py"


rule merge_ezvir_outputs:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: expand("Ezvir/{sample}/report/ALL-RESULTS.ezv",sample=list(read_naming.keys()))

    output: "benchmark_tools/tables/ezvir/all_ezvir.tsv"

    params: read_len=config["samples_read_length"]

    script: "scripts/Ezvir-merge.py"


rule merge_ganon_outputs:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: vir=expand("ganon/{sample}/viruses/classified.rep",sample=list(read_naming.keys())),
           bac=expand("ganon/{sample}/bacteria/classified.rep",sample=list(read_naming.keys()))

    output: "benchmark_tools/tables/ganon/all_ganon.tsv"

    script: "scripts/ganon-merge.py"



rule generate_heatmaps:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: "benchmark_tools/tables/{tool}/all_{tool}.tsv"

    output: "benchmark_tools/heatmaps/all_{tool}.pdf"

    params: rank=config["target_rank"],
            value=config["target_value"]

    script: "scripts/heatmap.py"

community=config['community_name']

rule genereate_gold_standard_profile:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: f"gold_standard/tables/{community}.tsv"

    output: "gold_standard/tables/gold_standard.tsv"

    script: "scripts/profile.py"



rule calculate_precision_recall_f1_score:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: gold_standard="gold_standard/tables/gold_standard.tsv",
            tool_out="benchmark_tools/tables/{tool}/all_{tool}.tsv"

    output: f"benchmark_tools/stats/{{tool}}/scores_{rank}_level.tsv"


    log: logging_folder + f"benchmark_tools/false_negatives/{rank}_level/{{tool}}.log"

    script: "scripts/stats.py"


rule regroup_tool_scores:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: expand(f"benchmark_tools/stats/{{tool}}/scores_{rank}_level.tsv",tool=['kraken2','bracken','pathseq','kaiju','surpi','ezvir','ganon'])

    output: f"benchmark_tools/stats/all_tools_{rank}_level.tsv"

    script: "scripts/concat.py"



