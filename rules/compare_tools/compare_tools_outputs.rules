
rule all_tools_output:
    input: merged_tsv=expand("benchmark_tools/tables/{tool}/all_{tool}.tsv",tool=['kraken2','bracken','pathseq','kaiju','surpi','ezvir','ganon']),
            all_hetmaps=expand("benchmark_tools/heatmaps/all_{tool}.pdf",tool=['kraken2','bracken','pathseq','kaiju','surpi','ezvir','ganon']),
            gold_standard_profile_files=expand("gold_standard/profile_files/{sample}.profile",sample=list(read_naming.keys())),
            sample_profile_files=expand("benchmark_tools/profile_files/{tool}/{sample}.profile",sample=list(read_naming.keys()),tool=['kraken2','bracken','pathseq','kaiju','surpi','ezvir','ganon']),
            stats=expand("benchmark_tools/stats/{tool}/{sample}/opal.txt",sample=list(read_naming.keys()),tool=['kraken2','bracken','pathseq','kaiju','surpi','ezvir','ganon'])
def paired(wildcards):
    fastq_list = read_naming[wildcards.sample]
    if len(fastq_list) == 2:
        return True
    else:
        return False

rule merge_Kraken2_outputs:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: expand("kraken2/{sample}/report.txt",sample=list(read_naming.keys()))

    output: "benchmark_tools/tables/kraken2/all_kraken2.tsv"

    script: "scripts/kraken2-merge.py"

rule merge_Bracken_outputs:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: expand("kraken2/{sample}/bracken.txt",sample=list(read_naming.keys()))

    output: "benchmark_tools/tables/bracken/all_bracken.tsv"

    script: "scripts/bracken-merge.py"

rule merge_Pathseq_outputs:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: expand("Pathseq/{sample}/output/scores.txt",sample=list(read_naming.keys()))

    output: "benchmark_tools/tables/pathseq/all_pathseq.tsv"

    script: "scripts/pathseq-merge.py"

rule merge_kaiju_outputs:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: expand("kaiju/{sample}/summary.tsv",sample=list(read_naming.keys()))

    output: "benchmark_tools/tables/kaiju/all_kaiju.tsv"

    log: logging_folder + "benchmark_tools/kaiju.log"

    script: "scripts/kaiju-merge.py"

rule merge_SURPI_outputs:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: bac=expand("SURPI/{sample}/OUTPUT_{sample}/{sample}.NT.snap.matched.d1.fl.Bacteria.annotated.species.clx.counttable",sample=list(read_naming.keys())),
           vir=expand("SURPI/{sample}/OUTPUT_{sample}/{sample}.NT.snap.matched.d16.fl.Viruses.filt.NTblastn_tru.dust.annotated.species.clx.counttable",sample=list(read_naming.keys()))
            #dust.annotated file because low complexity regions are substracted

    output: "benchmark_tools/tables/surpi/all_surpi.tsv"

    log: logging_folder + "benchmark_tools/surpi.log"

    script: "scripts/surpi-merge.py"


rule merge_ezvir_outputs:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: expand("Ezvir/{sample}/report/ALL-RESULTS.ezv",sample=list(read_naming.keys()))

    output: "benchmark_tools/tables/ezvir/all_ezvir.tsv"

    params: read_len=config["samples_read_length"]

    script: "scripts/Ezvir-merge.py"


rule merge_ganon_outputs:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: vir=expand("ganon/{sample}/viruses/classified.rep",sample=list(read_naming.keys())),
           #human=expand("ganon/{sample}/human/classified.rep",sample=list(read_naming.keys())),
           bac=expand("ganon/{sample}/bacteria/classified.rep",sample=list(read_naming.keys()))

    output: "benchmark_tools/tables/ganon/all_ganon.tsv"

    script: "scripts/ganon-merge.py"



rule generate_heatmaps:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: "benchmark_tools/tables/{tool}/all_{tool}.tsv"

    output: "benchmark_tools/heatmaps/all_{tool}.pdf"

    params: rank=config["target_rank"],
            value=config["target_value"]

    script: "scripts/heatmap.py"

rule genereate_gold_standard_profiles:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: "gold_standard/tables/{sample}.tsv"

    output: "gold_standard/profile_files/{sample}.profile"

    script: "scripts/profile.py"


rule genereate_sample_profiles:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: "benchmark_tools/tables/{tool}/{sample}.tsv"

    output: "benchmark_tools/profile_files/{tool}/{sample}.profile"

    script: "scripts/sample-profile.py"

rule get_stats:
    conda: pipeline_path + "envs/compare-tools.yml"

    input: gold_standard="gold_standard/profile_files/{sample}.profile",
             sample_profile="benchmark_tools/profile_files/{tool}/{sample}.profile"

    output: "benchmark_tools/stats/{tool}/{sample}/opal.txt"

    params: "benchmark_tools/stats/{tool}/{sample}"

    log: logging_folder + "benchmark_tools/stats/{tool}/{sample}.log"

    shell:
        """
        opal.py -g {input.gold_standard} -o {params} {input.sample_profile}
        """